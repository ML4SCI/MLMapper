{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83832900",
   "metadata": {},
   "outputs": [],
   "source": [
    "#expand cell width to 100%\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276ccc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas \n",
    "import pandas as pd\n",
    "#import numpy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3400b51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the address of the dataframe containing the pre-processed dataset\n",
    "dataFramePickleAddress=\"C:/ML4Sci/Ml4Sci_GRS_abundance_estimation/Dataset/GRSFiveDegreeSectionPreProcessedDataset.pkl\"\n",
    "#dataFramePickleAddress=\"D:/Non-academic/GSOC23/Ml4Sci_GRS_abundance_estimation/Dataset/GRSFiveDegreeSectionPreProcessedDataset.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b151d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the dataframe\n",
    "dataframe=pd.read_pickle(dataFramePickleAddress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ac4239",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the dataframe\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79629eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a variable epsilon, which contains a very small value that is added to all inputs and outputs to make sure no values are 0\n",
    "epsilon=1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00ab13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the abundance values for the seven elements for which the model is to be trained\n",
    "#aluminium\n",
    "aluminiumAbundances=dataframe['Aluminum'].to_numpy()+epsilon\n",
    "#calcium\n",
    "calciumAbundances=dataframe['Calcium'].to_numpy()+epsilon\n",
    "#iron\n",
    "ironAbundances=dataframe['Iron'].to_numpy()+epsilon\n",
    "#magnesium\n",
    "magnesiumAbundances=dataframe['Magnesium'].to_numpy()+epsilon\n",
    "#oxygen\n",
    "oxygenAbundances=dataframe['Oxygen'].to_numpy()+epsilon\n",
    "#silicon\n",
    "siliconAbundances=dataframe['Silicon'].to_numpy()+epsilon\n",
    "#titanium\n",
    "titaniumAbundances=dataframe['Titanium'].to_numpy()+epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3460a907",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get abundance values for the three nuclear elements\n",
    "potassiumAbundances=dataframe['Potassium'].to_numpy()+epsilon\n",
    "thoriumAbundances=dataframe['Thorium'].to_numpy()+epsilon\n",
    "uraniumAbundances=dataframe['Uranium'].to_numpy()+epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8638aab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale nuclear abundance values between 0 and 1\n",
    "potassiumAbundances=(potassiumAbundances-np.amin(potassiumAbundances))/np.ptp(potassiumAbundances)\n",
    "thoriumAbundances=(thoriumAbundances-np.amin(thoriumAbundances))/np.ptp(thoriumAbundances)\n",
    "uraniumAbundances=(uraniumAbundances-np.amin(uraniumAbundances))/np.ptp(uraniumAbundances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78ae584",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an list which contains the element names to be estimated\n",
    "regularElementNames=['Aluminum',\n",
    "                     'Calcium',\n",
    "                     'Iron',\n",
    "                     'Magnesium',\n",
    "                     'Oxygen',\n",
    "                     'Silicon',\n",
    "                     'Titanium']\n",
    "#conver the list to a numpy array\n",
    "regularElementNames=np.array(regularElementNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f5325e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine the elemental abundances into a single matrix\n",
    "regularElementalAbundances=np.dstack([aluminiumAbundances,\n",
    "                                      calciumAbundances,\n",
    "                                      ironAbundances,\n",
    "                                      magnesiumAbundances,\n",
    "                                      oxygenAbundances,\n",
    "                                      siliconAbundances,\n",
    "                                      titaniumAbundances])\n",
    "#reshape the abundance matrix\n",
    "regularElementalAbundances=regularElementalAbundances[0,:,:]\n",
    "#rescale weigth percent values from % (0-100) to franctions (0-1)\n",
    "regularElementalAbundances=regularElementalAbundances/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7dd49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an list which contains the element names to be estimated\n",
    "nuclearElementNames=['Potassium',\n",
    "                     'Thorium',\n",
    "                     'Uranium']\n",
    "#conver the list to a numpy array\n",
    "nuclearElementNames=np.array(nuclearElementNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33eee1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an array containing the names of all elements\n",
    "allElementNames=np.hstack([regularElementNames,nuclearElementNames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca4b854",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine the elemental abundances into a single matrix\n",
    "nuclearElementalAbundances=np.dstack([potassiumAbundances,\n",
    "                                      thoriumAbundances,\n",
    "                                      uraniumAbundances])\n",
    "#reshape the abundance matrix\n",
    "nuclearElementalAbundances=nuclearElementalAbundances[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e32348",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the pre-processed spectra as a numpy array\n",
    "preprocessedSpectra=dataframe['Normalized Continuum Removed Denoised Log Scaled Spectra'].to_numpy()\n",
    "#reshape the numpy array\n",
    "preprocessedSpectra=np.vstack(preprocessedSpectra)+epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6d40e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the eneergy values for each band\n",
    "gain=17.8 #keV/channel\n",
    "energyBands=np.arange(0,512,1)*gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa371af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the index of the last relavent band\n",
    "finalRelaventBandIndex=np.argmin(np.abs(energyBands-8000))\n",
    "#compute the index of the first relavent band\n",
    "firstRelaventBandIndex=finalRelaventBandIndex-preprocessedSpectra.shape[1]+1\n",
    "#get the energies of the relavent bands\n",
    "relaventEnergyBands=energyBands[firstRelaventBandIndex:finalRelaventBandIndex+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5cf87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the variables no longer needed from memory\n",
    "del firstRelaventBandIndex\n",
    "del finalRelaventBandIndex\n",
    "del energyBands\n",
    "del gain\n",
    "del dataframe\n",
    "del dataFramePickleAddress\n",
    "del aluminiumAbundances\n",
    "del calciumAbundances\n",
    "del ironAbundances\n",
    "del magnesiumAbundances\n",
    "del oxygenAbundances\n",
    "del siliconAbundances\n",
    "del titaniumAbundances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aabc366",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pyplot from matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0badb81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set plot parameters\n",
    "baseFontSize=18\n",
    "noOfBinsForHistogram=100\n",
    "noOfXticks=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31fb567",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a figure \n",
    "figure,axes=plt.subplots(nrows=1,\n",
    "                         ncols=7,\n",
    "                         figsize=(35,5),\n",
    "                         dpi=200)\n",
    "#iterate over all elements\n",
    "for i in range(regularElementNames.shape[0]):\n",
    "    #plot the histogram\n",
    "    temp=axes[i].hist(regularElementalAbundances[:,i],\n",
    "                      bins=noOfBinsForHistogram)\n",
    "    #set the title of the figure\n",
    "    axes[i].set_title(regularElementNames[i],fontsize=baseFontSize*1.2)\n",
    "    #set the axis labels\n",
    "    axes[i].set_xlabel(\"Wt frac.\",fontsize=baseFontSize*1.2)\n",
    "    axes[i].set_ylabel(\"Freq\",fontsize=baseFontSize*1.2)\n",
    "    #set the ticks and their label sizes\n",
    "    axes[i].set_xticks(np.arange(np.amin(regularElementalAbundances[:,i]),\n",
    "                                 np.amax(regularElementalAbundances[:,i])+np.ptp(regularElementalAbundances[:,i])/noOfXticks,\n",
    "                                 np.ptp(regularElementalAbundances[:,i])/noOfXticks),\n",
    "                       labels=np.round(np.arange(np.amin(regularElementalAbundances[:,i]),\n",
    "                                                 np.amax(regularElementalAbundances[:,i])+np.ptp(regularElementalAbundances[:,i])/noOfXticks,\n",
    "                                                 np.ptp(regularElementalAbundances[:,i])/noOfXticks),\n",
    "                                       2),\n",
    "                       fontsize=baseFontSize)\n",
    "    #set the margins\n",
    "    axes[i].margins(0.01)\n",
    "    \n",
    "#add a title\n",
    "figure.suptitle(\"Distribution of abundance values\",\n",
    "                fontsize=baseFontSize*1.5)\n",
    "#adjust the layout\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c10303d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a figure \n",
    "figure,axes=plt.subplots(nrows=1,\n",
    "                         ncols=3,\n",
    "                         figsize=(15,5),\n",
    "                         dpi=200)\n",
    "#iterate over all elements\n",
    "for i in range(nuclearElementNames.shape[0]):\n",
    "    #plot the histogram\n",
    "    temp=axes[i].hist(nuclearElementalAbundances[:,i],\n",
    "                      bins=noOfBinsForHistogram)\n",
    "    #set the title of the figure\n",
    "    axes[i].set_title(nuclearElementNames[i],fontsize=baseFontSize*1.2)\n",
    "    #set the axis labels\n",
    "    axes[i].set_xlabel(\"PPM\",fontsize=baseFontSize*1.2)\n",
    "    axes[i].set_ylabel(\"Freq\",fontsize=baseFontSize*1.2)\n",
    "    #set the ticks and their label sizes\n",
    "    axes[i].set_xticks(np.arange(np.amin(nuclearElementalAbundances[:,i]),\n",
    "                                 np.amax(nuclearElementalAbundances[:,i])+np.ptp(nuclearElementalAbundances[:,i])/noOfXticks,\n",
    "                                 np.ptp(nuclearElementalAbundances[:,i])/noOfXticks),\n",
    "                       labels=np.round(np.arange(np.amin(nuclearElementalAbundances[:,i]),\n",
    "                                                 np.amax(nuclearElementalAbundances[:,i])+np.ptp(nuclearElementalAbundances[:,i])/noOfXticks,\n",
    "                                                 np.ptp(nuclearElementalAbundances[:,i])/noOfXticks),\n",
    "                                       2),\n",
    "                       fontsize=baseFontSize)\n",
    "    #set the margins\n",
    "    axes[i].margins(0.01)\n",
    "    \n",
    "#add a title\n",
    "figure.suptitle(\"Distribution of abundance values\",\n",
    "                fontsize=baseFontSize*1.5)\n",
    "#adjust the layout\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f21cd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a figure\n",
    "plt.figure(figsize=(30,15),\n",
    "           dpi=100)\n",
    "#and plot all the spectra\n",
    "for i in range(preprocessedSpectra.shape[0]):\n",
    "    plt.plot(relaventEnergyBands,\n",
    "             preprocessedSpectra[i,:],\n",
    "             lw=5)\n",
    "#annotate the figure\n",
    "plt.title(\"Preprocessed GRS Spectra\",\n",
    "          fontsize=baseFontSize*1.8)\n",
    "plt.xticks(np.arange(np.amin(relaventEnergyBands),\n",
    "                     np.amax(relaventEnergyBands)+np.ptp(relaventEnergyBands)/noOfXticks,\n",
    "                     np.ptp(relaventEnergyBands)/noOfXticks),\n",
    "           fontsize=baseFontSize*1.2)\n",
    "plt.yticks(np.arange(np.amin(preprocessedSpectra),\n",
    "                     np.amax(preprocessedSpectra)+np.ptp(preprocessedSpectra)/noOfXticks,\n",
    "                     np.ptp(preprocessedSpectra)/noOfXticks),\n",
    "           labels=np.round(np.arange(np.amin(preprocessedSpectra),\n",
    "                                     np.amax(preprocessedSpectra)+np.ptp(preprocessedSpectra)/noOfXticks,\n",
    "                                     np.ptp(preprocessedSpectra)/noOfXticks),\n",
    "                           2),\n",
    "           fontsize=baseFontSize*1.2)\n",
    "plt.xlabel(\"KeV\",\n",
    "           fontsize=baseFontSize*1.5)\n",
    "plt.ylabel(\"log(Counts/min) ratioed\",\n",
    "           fontsize=baseFontSize*1.5)\n",
    "plt.margins(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f788bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import PCA from sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "#import cosine distance from scipy\n",
    "from scipy.spatial.distance import cosine as cosineDistance\n",
    "\n",
    "#define a function which given an array of spectra, explained-variance ratio, and no of spectra to be selected\n",
    "#returns the indices of most interesting (most unlike the others) spectra\n",
    "def extractMostExtremeSpectraWithDEMUD(spectra,varianceToExplain,noOfSpectraToRetrive):\n",
    "    #create an array to hold the indices of the selected (interesting) super-pixels\n",
    "    selectedSpectraIndices=[]\n",
    "    #create a PCA object which explains over 95% of the variance in the data\n",
    "    pcaObject=PCA(n_components=varianceToExplain,\n",
    "                  svd_solver='auto')\n",
    "    #compute the PCA model for the spectra and use it to reconstruct the orginal spectra\n",
    "    reconstructedSpectra=pcaObject.inverse_transform(pcaObject.fit_transform(spectra))\n",
    "    #create an array to save the reconstruction error (cosine distance)\n",
    "    reconstructionErrors=np.full(spectra.shape[0],\n",
    "                                 np.nan)\n",
    "    #compute the cosine distance between the orginal and reconstructed spectra\n",
    "    for i in range(spectra.shape[0]):\n",
    "        reconstructionErrors[i]=cosineDistance(spectra[i,:],\n",
    "                                               reconstructedSpectra[i,:])\n",
    "    #save the index of the spectra with the greatest error\n",
    "    selectedSpectraIndices.append(np.argmax(reconstructionErrors))\n",
    "\n",
    "    #iteratively extract the most dissimar spectra\n",
    "    for i in range(noOfSpectraToRetrive-1):\n",
    "        #fit PCA to the selected spectra\n",
    "        pcaObject.fit(spectra[selectedSpectraIndices,:])\n",
    "        #apply PCA and then reconstruct the spectra\n",
    "        reconstructedSpectra=pcaObject.inverse_transform(pcaObject.transform(spectra))\n",
    "        #create an array to save the reconstruction error (cosine distance)\n",
    "        reconstructionErrors=np.full(spectra.shape[0],np.nan)\n",
    "        #compute the cosine distance between the orginal and reconstructed spectra\n",
    "        for i in range(spectra.shape[0]):\n",
    "            reconstructionErrors[i]=cosineDistance(spectra[i,:],\n",
    "                                                   reconstructedSpectra[i,:])\n",
    "        #remove the error of the selected pixels\n",
    "        reconstructionErrors=np.delete(reconstructionErrors,\n",
    "                                       selectedSpectraIndices)\n",
    "        #get the index of the spectra with the greatest error\n",
    "        selectedSpectraIndices.append(np.delete(np.arange(0,spectra.shape[0],1),\n",
    "                                                selectedSpectraIndices,axis=0)[np.argmax(reconstructionErrors)])\n",
    "        \n",
    "    return selectedSpectraIndices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61792c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set parameters for extracting differing spectra\n",
    "varianceToBeExplainedDuringDEMUD=0.95\n",
    "noOfExtremeSpectraToBeRetrieved=179\n",
    "#get indices of the most extreme spectra\n",
    "extremeIndices=extractMostExtremeSpectraWithDEMUD(preprocessedSpectra,\n",
    "                                                  varianceToExplain=varianceToBeExplainedDuringDEMUD,\n",
    "                                                  noOfSpectraToRetrive=noOfExtremeSpectraToBeRetrieved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50b6ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the subset data\n",
    "#spectra\n",
    "preprocessedSpectra=preprocessedSpectra[extremeIndices,:]\n",
    "#regular abundances\n",
    "regularElementalAbundances=regularElementalAbundances[extremeIndices,:]\n",
    "#nuclear abundances\n",
    "nuclearElementalAbundances=nuclearElementalAbundances[extremeIndices,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ce3ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a figure \n",
    "figure,axes=plt.subplots(nrows=1,\n",
    "                         ncols=7,\n",
    "                         figsize=(35,5),\n",
    "                         dpi=200)\n",
    "#iterate over all elements\n",
    "for i in range(regularElementNames.shape[0]):\n",
    "    #plot the histogram\n",
    "    temp=axes[i].hist(regularElementalAbundances[:,i],\n",
    "                      bins=noOfBinsForHistogram)\n",
    "    #set the title of the figure\n",
    "    axes[i].set_title(regularElementNames[i],fontsize=baseFontSize*1.2)\n",
    "    #set the axis labels\n",
    "    axes[i].set_xlabel(\"Wt frac.\",fontsize=baseFontSize*1.2)\n",
    "    axes[i].set_ylabel(\"Freq\",fontsize=baseFontSize*1.2)\n",
    "    #set the ticks and their label sizes\n",
    "    axes[i].set_xticks(np.arange(np.amin(regularElementalAbundances[:,i]),\n",
    "                                 np.amax(regularElementalAbundances[:,i])+np.ptp(regularElementalAbundances[:,i])/noOfXticks,\n",
    "                                 np.ptp(regularElementalAbundances[:,i])/noOfXticks),\n",
    "                       labels=np.round(np.arange(np.amin(regularElementalAbundances[:,i]),\n",
    "                                                 np.amax(regularElementalAbundances[:,i])+np.ptp(regularElementalAbundances[:,i])/noOfXticks,\n",
    "                                                 np.ptp(regularElementalAbundances[:,i])/noOfXticks),\n",
    "                                       2),\n",
    "                       fontsize=baseFontSize)\n",
    "    #set the margins\n",
    "    axes[i].margins(0.01)\n",
    "    \n",
    "#add a title\n",
    "figure.suptitle(\"Distribution of abundance values\",\n",
    "                fontsize=baseFontSize*1.5)\n",
    "#adjust the layout\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f49b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a figure \n",
    "figure,axes=plt.subplots(nrows=1,\n",
    "                         ncols=3,\n",
    "                         figsize=(15,5),\n",
    "                         dpi=200)\n",
    "#iterate over all elements\n",
    "for i in range(nuclearElementNames.shape[0]):\n",
    "    #plot the histogram\n",
    "    temp=axes[i].hist(nuclearElementalAbundances[:,i],\n",
    "                      bins=noOfBinsForHistogram)\n",
    "    #set the title of the figure\n",
    "    axes[i].set_title(nuclearElementNames[i],fontsize=baseFontSize*1.2)\n",
    "    #set the axis labels\n",
    "    axes[i].set_xlabel(\"PPM\",fontsize=baseFontSize*1.2)\n",
    "    axes[i].set_ylabel(\"Freq\",fontsize=baseFontSize*1.2)\n",
    "    #set the ticks and their label sizes\n",
    "    axes[i].set_xticks(np.arange(np.amin(nuclearElementalAbundances[:,i]),\n",
    "                                 np.amax(nuclearElementalAbundances[:,i])+np.ptp(nuclearElementalAbundances[:,i])/noOfXticks,\n",
    "                                 np.ptp(nuclearElementalAbundances[:,i])/noOfXticks),\n",
    "                       labels=np.round(np.arange(np.amin(nuclearElementalAbundances[:,i]),\n",
    "                                                 np.amax(nuclearElementalAbundances[:,i])+np.ptp(nuclearElementalAbundances[:,i])/noOfXticks,\n",
    "                                                 np.ptp(nuclearElementalAbundances[:,i])/noOfXticks),\n",
    "                                       2),\n",
    "                       fontsize=baseFontSize)\n",
    "    #set the margins\n",
    "    axes[i].margins(0.01)\n",
    "    \n",
    "#add a title\n",
    "figure.suptitle(\"Distribution of abundance values\",\n",
    "                fontsize=baseFontSize*1.5)\n",
    "#adjust the layout\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8937b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a figure\n",
    "plt.figure(figsize=(30,15),\n",
    "           dpi=100)\n",
    "#and plot all the spectra\n",
    "for i in range(preprocessedSpectra.shape[0]):\n",
    "    plt.plot(relaventEnergyBands,\n",
    "             preprocessedSpectra[i,:],\n",
    "             lw=5)\n",
    "#annotate the figure\n",
    "plt.title(\"Preprocessed GRS Spectra\",\n",
    "          fontsize=baseFontSize*1.8)\n",
    "plt.xticks(np.arange(np.amin(relaventEnergyBands),\n",
    "                     np.amax(relaventEnergyBands)+np.ptp(relaventEnergyBands)/noOfXticks,\n",
    "                     np.ptp(relaventEnergyBands)/noOfXticks),\n",
    "           fontsize=baseFontSize*1.2)\n",
    "plt.yticks(np.arange(np.amin(preprocessedSpectra),\n",
    "                     np.amax(preprocessedSpectra)+np.ptp(preprocessedSpectra)/noOfXticks,\n",
    "                     np.ptp(preprocessedSpectra)/noOfXticks),\n",
    "           labels=np.round(np.arange(np.amin(preprocessedSpectra),\n",
    "                                     np.amax(preprocessedSpectra)+np.ptp(preprocessedSpectra)/noOfXticks,\n",
    "                                     np.ptp(preprocessedSpectra)/noOfXticks),\n",
    "                           2),\n",
    "           fontsize=baseFontSize*1.2)\n",
    "plt.xlabel(\"KeV\",\n",
    "           fontsize=baseFontSize*1.5)\n",
    "plt.ylabel(\"log(Counts/min) ratioed\",\n",
    "           fontsize=baseFontSize*1.5)\n",
    "plt.margins(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e062271",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84f5f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set a global seed value\n",
    "globalSeed=23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7bbff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the number of channels in the pre-processed spectra\n",
    "noOfChannels=preprocessedSpectra.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36a7466",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an input layer\n",
    "inputLayer=tf.keras.Input(shape=(noOfChannels,\n",
    "                                 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b649913b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a funtion which creates the CAM (Channel Attention Module)\n",
    "def createCAM(inputFeatureBlock,reductionRatio):\n",
    "    #perform max pooling along the channel dimension\n",
    "    channelMaxPooledFeatures=tf.math.reduce_max(inputFeatureBlock,\n",
    "                                                axis=1,\n",
    "                                                keepdims=False)\n",
    "    \n",
    "    #perform avg pooling along the channel dimension\n",
    "    channelAvgPooledFeatures=tf.math.reduce_mean(inputFeatureBlock,\n",
    "                                                 axis=1,\n",
    "                                                 keepdims=False)\n",
    "    \n",
    "    #create the bottleneck for the MLP\n",
    "    bottleneckLayer=tf.keras.layers.Dense(channelAvgPooledFeatures.shape[-1]//reductionRatio,\n",
    "                                          activation='relu')\n",
    "    \n",
    "    #create the recontruction layer for the MLP\n",
    "    outputLayer=tf.keras.layers.Dense(channelAvgPooledFeatures.shape[-1],\n",
    "                                      activation='relu')\n",
    "    \n",
    "    \n",
    "    #pass the max pooled features through the bottle-neck\n",
    "    reconstructeedMaxPooledFeatures=outputLayer(bottleneckLayer(channelMaxPooledFeatures))\n",
    "    \n",
    "    \n",
    "    #pass the avg pooled features through the bottle-neck\n",
    "    reconstructeedAvgPooledFeatures=outputLayer(bottleneckLayer(channelMaxPooledFeatures))\n",
    "    \n",
    "    \n",
    "    #add the two reconstructed features together\n",
    "    summedFeatures=tf.math.add(reconstructeedMaxPooledFeatures,\n",
    "                               reconstructeedAvgPooledFeatures)\n",
    "    \n",
    "    #apply sigmoid activation to the summed features to get the channel attention map\n",
    "    channelAttentionMap=tf.keras.activations.sigmoid(summedFeatures)\n",
    "    \n",
    "    #return the channel attention map\n",
    "    return channelAttentionMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367188b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function to create the Spatial attention module (SAM)\n",
    "def createSAM(inputFeatureBlock,kernelSize):\n",
    "    #perform max pooling on the input features\n",
    "    maxPooledFeatureMap=tf.math.reduce_max(inputFeatureBlock,\n",
    "                                           axis=-1,\n",
    "                                           keepdims=False)\n",
    "    #perform average pooling on the input features\n",
    "    averagePooledFeatureMap=tf.math.reduce_mean(inputFeatureBlock,\n",
    "                                                 axis=-1,\n",
    "                                                 keepdims=False)\n",
    "    \n",
    "    #concatenate the feature maps together\n",
    "    concatenatedFeatureMaps=tf.concat([tf.expand_dims(maxPooledFeatureMap,\n",
    "                                                      axis=-1),\n",
    "                                       tf.expand_dims(averagePooledFeatureMap,\n",
    "                                                      axis=-1)],\n",
    "                                      axis=-1)\n",
    "    \n",
    "    #create the convolutional layer to be applied to the concatenated feature map\n",
    "    convolutionLayer=tf.keras.layers.Conv1D(filters=1,\n",
    "                                            kernel_size=kernelSize,\n",
    "                                            strides=1,\n",
    "                                            padding='same',\n",
    "                                            activation='sigmoid')\n",
    "    \n",
    "    \n",
    "    #get the spatial attention map\n",
    "    spatialAttentionMap=convolutionLayer(concatenatedFeatureMaps)\n",
    "    \n",
    "    \n",
    "    #return the channel attention map\n",
    "    return spatialAttentionMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e5503d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function which creates a CBAM block\n",
    "def createCBAM(inputLayer,convolutionalKernelSize,noOfConvolutionalFilters,reductionRatio,spatialKernelSize):\n",
    "    #create a convolutional layer\n",
    "    convolutionalLayer=tf.keras.layers.Conv1D(filters=noOfConvolutionalFilters,\n",
    "                                              kernel_size=convolutionalKernelSize,\n",
    "                                              strides=1,\n",
    "                                              padding='same')\n",
    "    #get the feature block from the convolutional layer\n",
    "    convolutionalFeatures=convolutionalLayer(inputLayer)\n",
    "    \n",
    "    #get the channel attention map\n",
    "    channelAttentionMap=createCAM(convolutionalFeatures,\n",
    "                                  reductionRatio)\n",
    "    \n",
    "    #replicate the channel attention to make it multiplicative with the features\n",
    "    replicatedChannelAttentionMaps=tf.expand_dims(channelAttentionMap,\n",
    "                                                  axis=1)\n",
    "    replicatedChannelAttentionMaps=tf.repeat(replicatedChannelAttentionMaps,\n",
    "                                             convolutionalFeatures.shape[1],\n",
    "                                             axis=1)\n",
    "    \n",
    "    #compute the channel refined feature by performing element-wise multiplication between the features and the channel attention maps\n",
    "    channelRefinedFeatures=tf.math.multiply(replicatedChannelAttentionMaps,\n",
    "                                            convolutionalFeatures)\n",
    "    \n",
    "    #get the spatial attention map\n",
    "    spatialAttentionMap=createSAM(channelRefinedFeatures,\n",
    "                                  spatialKernelSize)\n",
    "    \n",
    "    #replicate the attention map to make it multiplicative with the channel-refined features\n",
    "    replicatedSpatialAttentionMaps=tf.repeat(spatialAttentionMap,\n",
    "                                             channelRefinedFeatures.shape[-1],\n",
    "                                             axis=-1)\n",
    "    \n",
    "    #multiply the attention map with the channel refined features\n",
    "    spatiallyRefinedFeatures=tf.math.multiply(replicatedSpatialAttentionMaps,\n",
    "                                              channelRefinedFeatures)\n",
    "    \n",
    "    \n",
    "    #add the refined features to the original features\n",
    "    refinedFeatures=tf.math.add(convolutionalFeatures,\n",
    "                                spatiallyRefinedFeatures)\n",
    "    \n",
    "    #return the refined features (i.e. the output of the CBAM)\n",
    "    return refinedFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a934369b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function which given a feature volume, applies a single convolutional block to it\n",
    "#the convolution block consists of\n",
    "#Convolutional layer; Activation; Batch Normalization; Dropout\n",
    "#the number of filters, their sizes, stride,, and dropout rate are specified\n",
    "def createConvolutionalBlock(inputVolume,noOfFilters,kernelSize,strideSize,dropoutRate,noOfCBAMBLocks):\n",
    "    #create a convolutional block\n",
    "    convolutionalLayer=tf.keras.layers.Conv1D(filters=noOfFilters,\n",
    "                                                   kernel_size=kernelSize,\n",
    "                                                   strides=strideSize,\n",
    "                                                   padding='valid')\n",
    "    #add the 1st Conv layer to the graph\n",
    "    volume=convolutionalLayer(inputVolume)\n",
    "\n",
    "    #apply Relu activation\n",
    "    preluActivation=tf.keras.layers.PReLU()\n",
    "    #add the 1st activation layer to the graph\n",
    "    volume=preluActivation(volume)\n",
    "\n",
    "    #apply batch normalization\n",
    "    batchNormalization=tf.keras.layers.BatchNormalization()\n",
    "    #add the 1st batch-norm layer to the graph\n",
    "    volume=batchNormalization(volume)\n",
    "\n",
    "    #apply dropout\n",
    "    dropoutLayer=tf.keras.layers.Dropout(dropoutRate,\n",
    "                                         noise_shape=None,\n",
    "                                         seed=globalSeed)\n",
    "    #add the 1st dropout layer to the graph\n",
    "    volume=dropoutLayer(volume)\n",
    "    \n",
    "    #add the specified number of CBAM blocks\n",
    "    for i in range(noOfCBAMBLocks):\n",
    "        #add the CBAM module\n",
    "        volume=createCBAM(volume,\n",
    "                          convolutionalKernelSize=kernelSize,\n",
    "                          noOfConvolutionalFilters=noOfFilters,\n",
    "                          reductionRatio=2,\n",
    "                          spatialKernelSize=3)\n",
    "        \n",
    "        \n",
    "        #apply batch norm over the refined features\n",
    "        batchNormalization=tf.keras.layers.BatchNormalization()\n",
    "        #add the 1st batch-norm layer to the graph\n",
    "        volume=batchNormalization(volume)\n",
    "        \n",
    "        #apply dropout\n",
    "        dropoutLayer=tf.keras.layers.Dropout(dropoutRate,\n",
    "                                             noise_shape=None,\n",
    "                                             seed=globalSeed)\n",
    "        #add the 1st dropout layer to the graph\n",
    "        volume=dropoutLayer(volume)\n",
    "    \n",
    "    return volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbe2dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list containing the number of features to be outputted by each Conv block\n",
    "noOfChannelsInEachConvBlock=[128,256,512,1024]\n",
    "#create a list containing the sizes filter for each Conv block\n",
    "filterSizesForEachConvBlock=[7,5,3,3]\n",
    "#create a list containing the strides for each Conv block\n",
    "strideSizesForEachConvBlock=[3,3,2,2]\n",
    "#create a list containing the dropout rate for each Conv block\n",
    "dropoutForEachConvBlock=[0.3,0.3,0.3,0.3]\n",
    "#create a list containing the number of CBAM blocks per convolutional block\n",
    "attentionBlocksPerConvolutionBlock=[4,4,4,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32313348",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a NN (Neural Network) graph containing just the input layer\n",
    "regularElementFeatures=inputLayer\n",
    "nuclearElementFeatures=inputLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0bc02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add Convolutional blocks to create the feature extractor for regular elements\n",
    "\n",
    "for i in range(len(noOfChannelsInEachConvBlock)):\n",
    "    #create a convolutional block\n",
    "    regularElementFeatures=createConvolutionalBlock(regularElementFeatures,\n",
    "                                                    noOfChannelsInEachConvBlock[i],\n",
    "                                                    filterSizesForEachConvBlock[i],\n",
    "                                                    strideSizesForEachConvBlock[i],\n",
    "                                                    dropoutForEachConvBlock[i],\n",
    "                                                    attentionBlocksPerConvolutionBlock[i])\n",
    "    \n",
    "    print(f\"Shape of the tensor outputted by the 0th Conv. Block {regularElementFeatures.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b61573",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flatten the features for the regular elemnents\n",
    "regularElementFeatures=tf.keras.layers.Flatten()(regularElementFeatures)\n",
    "print(f\"Shape of flattened features {regularElementFeatures.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3e3d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flatten the features for the regular elemnents\n",
    "regularElementFeatures=tf.keras.layers.Flatten()(regularElementFeatures)\n",
    "print(f\"Shape of flattened features {regularElementFeatures.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788f7c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list containing the number of features to be outputted by each Conv block\n",
    "noOfChannelsInEachConvBlock=[128,256,512,1024]\n",
    "#create a list containing the sizes filter for each Conv block\n",
    "filterSizesForEachConvBlock=[7,5,3,3]\n",
    "#create a list containing the strides for each Conv block\n",
    "strideSizesForEachConvBlock=[3,3,2,2]\n",
    "#create a list containing the dropout rate for each Conv block\n",
    "dropoutForEachConvBlock=[0.3,0.3,0.3,0.3]\n",
    "#create a list containing the number of CBAM blocks per convolutional block\n",
    "attentionBlocksPerConvolutionBlock=[2,2,2,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784f4108",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add Convolutional blocks to create the feature extractor for regular elements\n",
    "\n",
    "for i in range(len(noOfChannelsInEachConvBlock)):\n",
    "    #create a convolutional block\n",
    "    nuclearElementFeatures=createConvolutionalBlock(nuclearElementFeatures,\n",
    "                                                    noOfChannelsInEachConvBlock[i],\n",
    "                                                    filterSizesForEachConvBlock[i],\n",
    "                                                    strideSizesForEachConvBlock[i],\n",
    "                                                    dropoutForEachConvBlock[i],\n",
    "                                                    attentionBlocksPerConvolutionBlock[i])\n",
    "    \n",
    "    print(f\"Shape of the tensor outputted by the 0th Conv. Block {nuclearElementFeatures.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df60f658",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flatten the features for the nuclear elements\n",
    "nuclearElementFeatures=tf.keras.layers.Flatten()(nuclearElementFeatures)\n",
    "print(f\"Shape of flattened features {nuclearElementFeatures.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65325566",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the regular elements' abundances from the generated features\n",
    "#it consists of three steps\n",
    "#1. Apply a dense layer with 7 nodes without any activation\n",
    "#2. Compute the absolute value of the values computed by dense layer\n",
    "#3. Compute the l1-norm of the 7 regular elements and and divide them by it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7679cd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function which given the output of a layer, ensures the sum of the values is one\n",
    "#it does this by computing the sum of the nodes and dividing each note by it\n",
    "def estimateAbundances(inputNodes,name):\n",
    "    sampleWiseSums=tf.keras.backend.sum(inputNodes,\n",
    "                                        axis=-1,\n",
    "                                        keepdims=True)\n",
    "    sampleWiseSums=tf.repeat(sampleWiseSums,\n",
    "                             sampleWiseSums.shape[-1],\n",
    "                             axis=-1)\n",
    "    return tf.math.divide(inputNodes,\n",
    "                          sampleWiseSums+1e-10,\n",
    "                          name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccce86a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a fully connected layer to the network (output layer)\n",
    "regularElementStage1=tf.keras.layers.Dense(7,activation=None)(regularElementFeatures)\n",
    "#compute absolute values\n",
    "regularElementStage2=tf.math.abs(regularElementStage1)\n",
    "#add a normalization layer to the network\n",
    "regularAbundanceEmbedding=estimateAbundances(regularElementStage2,\n",
    "                                             name=\"Regular_Abundances\")\n",
    "\n",
    "\n",
    "#print shape of abundances\n",
    "print(f\"Unscaled regular element shape {regularAbundanceEmbedding.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9df6ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the abundances for the nuclear elements from their features\n",
    "nuclearElementStage1=tf.keras.layers.Dense(3,activation=None)(nuclearElementFeatures)\n",
    "#compute absolute values\n",
    "nuclearAbundanceEmbedding=tf.math.abs(nuclearElementStage1,name=\"Nuclear_Abundances\")\n",
    "#print shape of abundances\n",
    "print(f\"Unscaled regular element shape {nuclearAbundanceEmbedding.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98a7d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concattenate the two abundances\n",
    "fullAbundanceEmbedding=tf.concat([regularAbundanceEmbedding,nuclearAbundanceEmbedding],\n",
    "                                 axis=-1)\n",
    "\n",
    "print(f\"All element shape {fullAbundanceEmbedding.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da089a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define regularization for the element-wise spectra\n",
    "#the regularizer encourages the elemental spectra to be different\n",
    "#this is done by computing the pairwise cosine distances, summing them up and maximizing this sum\n",
    "\n",
    "#as these weights are from seperate (parallel)layers this regularization could not be implemented as inheriting the kernel regularization class\n",
    "#therefore it is implemented as a loss. As such it recieves the true and predicted values but ignores them\n",
    "class spectralDisimilarityRegularization(tf.keras.losses.Loss):\n",
    "    def call(self,linearLayerWeights,_):\n",
    "        #normalize the weights\n",
    "        linearLayerWeights=tf.math.l2_normalize(linearLayerWeights,\n",
    "                                                axis=-2,\n",
    "                                                epsilon=1e-1)\n",
    "\n",
    "        #multiply the weights by their transpose to get the dot products (provided the weight vectors have an l2-norm of 1)\n",
    "        #sum the dot products up to get a value proportional to the sum of pairwise dot products\n",
    "        unmixingSpectralSimilarity=tf.math.reduce_sum(tf.linalg.matmul(linearLayerWeights,\n",
    "                                                                       linearLayerWeights,\n",
    "                                                                       transpose_b=True))\n",
    "        #compute sum of cosine similarities by subtracting the number of sames from the data and then dividing by half\n",
    "        unmixingSpectralSimilarity=(unmixingSpectralSimilarity-linearLayerWeights.shape[-1])/2\n",
    "\n",
    "        #return the similarity estimate which is the regularization value\n",
    "        return unmixingSpectralSimilarity\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d543348",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a composite constraint to be applied to the weights of the hidden layers which are the weights\n",
    "class UnitNormNonNegetivityConstraint(tf.keras.constraints.Constraint):\n",
    "    def __init__(self,axis=0):\n",
    "        self.axis=axis\n",
    "    def __call__(self, w):\n",
    "        #apply positivity constraint\n",
    "        w=w*tf.cast(tf.greater_equal(w,0.0),\n",
    "                    tf.keras.backend.floatx())\n",
    "        \n",
    "        return w\n",
    "\n",
    "\n",
    "\n",
    "#create the composite constraint by combining the two constraints\n",
    "unitNormNonNegetivityConstraint=UnitNormNonNegetivityConstraint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5863ada9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an input layer which consists of the number one, it will be used to get the learnt spectra\n",
    "unitNodeLayer=tf.keras.Input(shape=(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfb1a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the linear hidden layer as a combination of ten hidden layers each connected to a single abundance value\n",
    "elementWiseHiddenLayers=[]\n",
    "abundanceScaledElementalSpectra=[]\n",
    "#create a list to hold the unscaled elemental spectra\n",
    "unscaledElementalSpectra=[]\n",
    "#create a list to hold the unscaled elemental spectra\n",
    "unscaledElementalSpectra=[]\n",
    "#iterate through each element\n",
    "for i in range(fullAbundanceEmbedding.shape[-1]):\n",
    "    #create a contraintedhidden layer to learn the current element's spectra\n",
    "    temp=tf.keras.layers.Dense(noOfChannels,\n",
    "                               activation=None,\n",
    "                               use_bias=False,\n",
    "                               kernel_constraint=unitNormNonNegetivityConstraint,\n",
    "                               name=allElementNames[i]+\"_Spectra\")\n",
    "    \n",
    "    \n",
    "    #save the layer to a list\n",
    "    elementWiseHiddenLayers.append(temp)\n",
    "    \n",
    "    \n",
    "    #multiply the layer by a the estimated abundance to scale the spectra by the abundance\n",
    "    temp1=temp(tf.expand_dims(fullAbundanceEmbedding[:,i],\n",
    "                             -1))\n",
    "    #save the learnt spectra scaled by the abundance\n",
    "    abundanceScaledElementalSpectra.append(temp1)\n",
    "    \n",
    "    \n",
    "    #multiply the layer by a the estimated abundance to scale the spectra by the abundance\n",
    "    temp2=temp(unitNodeLayer)\n",
    "    #save the learnt spectra\n",
    "    unscaledElementalSpectra.append(temp2)\n",
    "    \n",
    "    #delete the temporary variable\n",
    "    del temp\n",
    "    print(f\"Shape of the spectra outputed by {allElementNames[i]} {abundanceScaledElementalSpectra[-1].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36f9228",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stack the abundance scaled elemental spectra together\n",
    "abundanceScaledElementalSpectra=tf.stack(abundanceScaledElementalSpectra,axis=-1)\n",
    "print(f\"Shape of tensor containing all ten elemental spectra scaled by abundance {abundanceScaledElementalSpectra.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0fbb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stack the abundance scaled elemental spectra together\n",
    "unscaledElementalSpectra=tf.stack(unscaledElementalSpectra,axis=-1)\n",
    "print(f\"Shape of tensor containing all ten elemental spectra {unscaledElementalSpectra.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559b91e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the element wise spectra\n",
    "linearMixedSpectra=tf.keras.backend.sum(abundanceScaledElementalSpectra,\n",
    "                                        axis=-1,\n",
    "                                        keepdims=False)\n",
    "print(f\"Shape of the linearly mixed spectra {linearMixedSpectra.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c387ff61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the model\n",
    "unmixingModel=tf.keras.Model(inputs=[inputLayer,\n",
    "                                     unitNodeLayer],\n",
    "                             outputs=[regularAbundanceEmbedding,\n",
    "                                      nuclearAbundanceEmbedding,\n",
    "                                      linearMixedSpectra,\n",
    "                                      unscaledElementalSpectra],\n",
    "                             name=\"Attention_Based_Regularized_Constrained_Dual_Encoder_Unmxing_Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adecd41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create loss functions for the embeddings, Mean Squared Error\n",
    "regularAbundanceLossFunction=tf.keras.losses.MeanSquaredError()\n",
    "nuclearAbundanceLossFunction=tf.keras.losses.MeanSquaredError()\n",
    "#create the loss function for the reconstructed spectra, Cosine loss\n",
    "recontructedSpectraLoss=tf.keras.losses.CosineSimilarity()\n",
    "#create the optimizer\n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "#set the number of epochs the model is to be trained for\n",
    "noOfEpochs=1000\n",
    "#set the batch size\n",
    "batchSize=16\n",
    "\n",
    "#create the regularizer\n",
    "spectralDisimilarityRegularizer=spectralDisimilarityRegularization()\n",
    "\n",
    "#steps the number of steps to be processed in one execution\n",
    "stepsPerExecution=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca099a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a loss dictionary\n",
    "lossDictionary=[regularAbundanceLossFunction,\n",
    "                nuclearAbundanceLossFunction,\n",
    "                recontructedSpectraLoss,\n",
    "                spectralDisimilarityRegularizer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f95c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creat a list with loss weights\n",
    "lossWeights=[25e-2,\n",
    "             1,\n",
    "             1e-3,\n",
    "             1e-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d21012",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the model\n",
    "unmixingModel.compile(optimizer=optimizer,\n",
    "                      loss=lossDictionary,\n",
    "                      loss_weights=lossWeights,\n",
    "                      steps_per_execution=stepsPerExecution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1504ae4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the address where the untrained model will be saved\n",
    "untrainedModelWeightsAddress=\"C:/ML4Sci/Models/untrained_Attention_Based_Regularized_Constrainted_Dual_Unmixing_Model_With_PReLU_Weights.h5\"\n",
    "#untrainedModelWeightsAddress=\"D:/Non-academic/GSOC23/untrained_Regularized_Constrainted_Dual_Unmixing_Model_With_PReLU_Weights_and_Linear_Decoder.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ceb1e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the untrained model\n",
    "unmixingModel.save_weights(untrainedModelWeightsAddress,\n",
    "                           overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39d796f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if the GPU is available\n",
    "if len(tf.config.list_physical_devices('GPU'))==1:\n",
    "    print(\"GPU available\")\n",
    "else:\n",
    "    print(\"GPU unavailable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f48c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "noOfXticks=3\n",
    "noOfYticks=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159d15f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an array to save the element-wise learnt spectra across folds\n",
    "learntElementalSpectra=np.zeros((noOfChannels,\n",
    "                                 fullAbundanceEmbedding.shape[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06dc351",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unmixingModel.load_weights(untrainedModelWeightsAddress)\n",
    "#compile the model\n",
    "unmixingModel.compile(optimizer=optimizer,\n",
    "                      loss=lossDictionary,\n",
    "                      loss_weights=lossWeights,\n",
    "                      steps_per_execution=stepsPerExecution)\n",
    "\n",
    "#fit the model to the current fold's data\n",
    "currentModelTrainingHistory=unmixingModel.fit(x=[preprocessedSpectra,\n",
    "                                                 np.ones(preprocessedSpectra.shape[0])\n",
    "                                                ],\n",
    "                                              y=[regularElementalAbundances,\n",
    "                                                 nuclearElementalAbundances,\n",
    "                                                 preprocessedSpectra,\n",
    "                                                 nuclearElementalAbundances],\n",
    "                                              batch_size=batchSize,\n",
    "                                              epochs=noOfEpochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc587cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the address where the untrained model will be saved\n",
    "#trainedModelWeightsAddress=\"D:/Non-academic/GSOC23/trained_Regularized_Constrainted_Dual_Unmixing_Model_With_PReLU_Weights_and_Linear_Decoder.h5\"\n",
    "trainedModelWeightsAddress=\"C:/ML4Sci/Models/trained_Attention_Based_Regularized_Constrainted_Dual_Unmixing_Model_With_PReLU_Weights_and_Linear_Decoder.h5\"\n",
    "\n",
    "#save the untrained model\n",
    "unmixingModel.save_weights(trainedModelWeightsAddress,\n",
    "                           overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df69328c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate through all the elements\n",
    "for i in range(len(allElementNames)):\n",
    "\n",
    "    #get the mean spectrum for the current element\n",
    "    currentElementSpectrum=unmixingModel.get_layer(allElementNames[i]+\"_Spectra\").get_weights()[0][0]\n",
    "\n",
    "    #save the spectrum\n",
    "    learntElementalSpectra[:,i]=currentElementSpectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2c63b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize the learnt spectra\n",
    "learntElementalSpectra=learntElementalSpectra/np.linalg.norm(learntElementalSpectra,axis=0,keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f5b661",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dictionary to hold energies corresponding to the peaks for each element\n",
    "elementalPeaks={}\n",
    "\n",
    "#create a temporary list with elemental peaks\n",
    "tempPeaks=[[843,1014,2211],[1942,3737],[846,1213,1809,7634],\n",
    "           [1369,1809],[6129],[1779],[983],\n",
    "           [1460],[584,2615],[609]]\n",
    "\n",
    "#iterate through all the elements and add their names (keys) and empty arrays (values) which will hold the peak energies to the dictionary\n",
    "for i in range(len(allElementNames)):\n",
    "    elementalPeaks[allElementNames[i]]=tempPeaks[i]\n",
    "    \n",
    "#delete the temporary array\n",
    "del tempPeaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7ac880",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a figure \n",
    "figure,axes=plt.subplots(nrows=2,\n",
    "                         ncols=5,\n",
    "                         figsize=(5*5,5*2),\n",
    "                         dpi=100,\n",
    "                         sharex=True,\n",
    "                         sharey=True)\n",
    "\n",
    "\n",
    "#iterate through all the elements\n",
    "for i in range(len(allElementNames)):\n",
    "\n",
    "    #get the name of the current element\n",
    "    currentElementName=allElementNames[i]\n",
    "\n",
    "    #get the mean spectrum for the current element\n",
    "    currentElementSpectrum=learntElementalSpectra[:,i]\n",
    "\n",
    "    #plot the peak postions\n",
    "    if len(elementalPeaks[currentElementName])>0:\n",
    "        axes[i//5,i%5].vlines(x=elementalPeaks[currentElementName],\n",
    "                              ymin=0,\n",
    "                              ymax=np.amax(currentElementSpectrum),\n",
    "                              colors='red',\n",
    "                              lw=2)\n",
    "        #add the position of the peaks as text\n",
    "        #for currentPeakPositon in elementalPeaks[currentElementName]:\n",
    "            #axes[i//5,i%5].text(x=currentPeakPositon,\n",
    "            #                    y=np.amax(currentElementSpectrum)+0.0001,\n",
    "            #                    s=str(currentPeakPositon),\n",
    "            #                    fontsize=baseFontSize*1)\n",
    "    \n",
    "    \n",
    "    #plot the spectrum\n",
    "    axes[i//5,i%5].bar(relaventEnergyBands,\n",
    "                       currentElementSpectrum,\n",
    "                       lw=10,\n",
    "                       width=10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #add the title\n",
    "    axes[i//5,i%5].set_title(currentElementName,\n",
    "                             fontsize=baseFontSize*1.2)\n",
    "\n",
    "    #set the margin\n",
    "    axes[i//5,i%5].margins(0.01)\n",
    "\n",
    "    #add xticks and label\n",
    "    if i//5==1:\n",
    "        axes[i//5,i%5].set_xticks(np.arange(np.amin(relaventEnergyBands),\n",
    "                                            np.amax(relaventEnergyBands)+np.ptp(relaventEnergyBands)/noOfXticks,\n",
    "                                            np.ptp(relaventEnergyBands)/noOfXticks),\n",
    "                                  labels=np.round(np.arange(np.amin(relaventEnergyBands),\n",
    "                                                            np.amax(relaventEnergyBands)+np.ptp(relaventEnergyBands)/noOfXticks,\n",
    "                                                            np.ptp(relaventEnergyBands)/noOfXticks),\n",
    "                                                  0).astype('int'),\n",
    "                                  fontsize=baseFontSize*1.1)\n",
    "        axes[i//5,i%5].set_xlabel(\"KeV\",fontsize=baseFontSize*1.2)\n",
    "\n",
    "    #add yticks and label\n",
    "    if i%5==0:\n",
    "        axes[i//5,i%5].set_ylabel(\"log(Counts/min) ratioed\",fontsize=baseFontSize*1.2)\n",
    "\n",
    "    axes[i//5,i%5].set_yticks(np.arange(0,\n",
    "                                np.amax(learntElementalSpectra)+np.ptp(learntElementalSpectra)/noOfYticks,\n",
    "                                np.ptp(learntElementalSpectra)/noOfYticks),\n",
    "                      labels=np.round(np.arange(0,\n",
    "                                                np.amax(learntElementalSpectra)+np.ptp(learntElementalSpectra)/noOfYticks,\n",
    "                                                np.ptp(learntElementalSpectra)/noOfYticks),\n",
    "                                      2),\n",
    "                      fontsize=baseFontSize*1.1)\n",
    "#add a title\n",
    "figure.suptitle(f\"Element-wise Learnt spectra\",\n",
    "                fontsize=baseFontSize*1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ba7669",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a figure \n",
    "figure,axes=plt.subplots(nrows=2,\n",
    "                         ncols=5,\n",
    "                         figsize=(5*5,5*2),\n",
    "                         dpi=100,\n",
    "                         sharex=True,\n",
    "                         sharey=True)\n",
    "\n",
    "\n",
    "#iterate through all the elements\n",
    "for i in range(len(allElementNames)):\n",
    "\n",
    "    #get the name of the current element\n",
    "    currentElementName=allElementNames[i]\n",
    "\n",
    "    #get the mean spectrum for the current element\n",
    "    currentElementSpectrum=learntElementalSpectra[:,i]\n",
    "\n",
    "    #plot the peak postions\n",
    "    if len(elementalPeaks[currentElementName])>0:\n",
    "        axes[i//5,i%5].vlines(x=elementalPeaks[currentElementName],\n",
    "                              ymin=0,\n",
    "                              ymax=np.amax(currentElementSpectrum),\n",
    "                              colors='red',\n",
    "                              lw=2)\n",
    "        #add the position of the peaks as text\n",
    "        #for currentPeakPositon in elementalPeaks[currentElementName]:\n",
    "            #axes[i//5,i%5].text(x=currentPeakPositon,\n",
    "            #                    y=np.amax(currentElementSpectrum)+0.0001,\n",
    "            #                    s=str(currentPeakPositon),\n",
    "            #                    fontsize=baseFontSize*1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #plot the spectrum\n",
    "    axes[i//5,i%5].plot(relaventEnergyBands,\n",
    "                        currentElementSpectrum,\n",
    "                        lw=2)\n",
    "    \n",
    "    \n",
    "    #add the title\n",
    "    axes[i//5,i%5].set_title(currentElementName,\n",
    "                             fontsize=baseFontSize*1.2)\n",
    "\n",
    "    #set the margin\n",
    "    axes[i//5,i%5].margins(0.01)\n",
    "\n",
    "    #add xticks and label\n",
    "    if i//5==1:\n",
    "        axes[i//5,i%5].set_xticks(np.arange(np.amin(relaventEnergyBands),\n",
    "                                            np.amax(relaventEnergyBands)+np.ptp(relaventEnergyBands)/noOfXticks,\n",
    "                                            np.ptp(relaventEnergyBands)/noOfXticks),\n",
    "                                  labels=np.round(np.arange(np.amin(relaventEnergyBands),\n",
    "                                                            np.amax(relaventEnergyBands)+np.ptp(relaventEnergyBands)/noOfXticks,\n",
    "                                                            np.ptp(relaventEnergyBands)/noOfXticks),\n",
    "                                                  0).astype('int'),\n",
    "                                  fontsize=baseFontSize*1.1)\n",
    "        axes[i//5,i%5].set_xlabel(\"KeV\",fontsize=baseFontSize*1.2)\n",
    "\n",
    "    #add yticks and label\n",
    "    if i%5==0:\n",
    "        axes[i//5,i%5].set_ylabel(\"log(Counts/min) ratioed\",fontsize=baseFontSize*1.2)\n",
    "\n",
    "    axes[i//5,i%5].set_yticks(np.arange(0,\n",
    "                                np.amax(learntElementalSpectra)+np.ptp(learntElementalSpectra)/noOfYticks,\n",
    "                                np.ptp(learntElementalSpectra)/noOfYticks),\n",
    "                      labels=np.round(np.arange(0,\n",
    "                                                np.amax(learntElementalSpectra)+np.ptp(learntElementalSpectra)/noOfYticks,\n",
    "                                                np.ptp(learntElementalSpectra)/noOfYticks),\n",
    "                                      2),\n",
    "                      fontsize=baseFontSize*1.1)\n",
    "#add a title\n",
    "figure.suptitle(f\"Element-wise Learnt spectra\",\n",
    "                fontsize=baseFontSize*1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4dbd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import uniform_filter from scipy\n",
    "from scipy.ndimage import uniform_filter1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb459cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#smooth the spectra using a mean filter\n",
    "smoothenedElementalSpectra=uniform_filter1d(learntElementalSpectra,\n",
    "                                            size=3,\n",
    "                                            axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abaf285",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an array to hold the smoothened element-wise spectra\n",
    "smoothenedElementWiseSpectra=np.zeros_like(learntElementalSpectra)\n",
    "\n",
    "#create a figure \n",
    "figure,axes=plt.subplots(nrows=2,\n",
    "                         ncols=5,\n",
    "                         figsize=(5*5,5*2),\n",
    "                         dpi=100,\n",
    "                         sharex=True,\n",
    "                         sharey=True)\n",
    "\n",
    "#iterate through the element-wise spectra and smoothen them\n",
    "for i in range(len(allElementNames)):\n",
    "    \n",
    "    #get the name of the current element\n",
    "    currentElementName=allElementNames[i]\n",
    "    \n",
    "    #get the current specturm\n",
    "    currentElementSpectrum=smoothenedElementalSpectra[:,i]\n",
    "    \n",
    "    \n",
    "    #plot the peak postions\n",
    "    if len(elementalPeaks[currentElementName])>0:\n",
    "        axes[i//5,i%5].vlines(x=elementalPeaks[currentElementName],\n",
    "                              ymin=0,\n",
    "                              ymax=np.amax(currentElementSpectrum),\n",
    "                              colors='red',\n",
    "                              lw=2)\n",
    "        #add the position of the peaks as text\n",
    "        #for currentPeakPositon in elementalPeaks[currentElementName]:\n",
    "            #axes[i//5,i%5].text(x=currentPeakPositon,\n",
    "            #                    y=np.amax(currentElementSpectrum)+0.0001,\n",
    "            #                    s=str(currentPeakPositon),\n",
    "            #                    fontsize=baseFontSize*1)\n",
    "    \n",
    "    #plot the spectrum\n",
    "    axes[i//5,i%5].plot(relaventEnergyBands,\n",
    "                        currentElementSpectrum,\n",
    "                        lw=2)\n",
    "    \n",
    "    \n",
    "    #add the title\n",
    "    axes[i//5,i%5].set_title(currentElementName,\n",
    "                             fontsize=baseFontSize*1.2)\n",
    "\n",
    "    #set the margin\n",
    "    axes[i//5,i%5].margins(0)\n",
    "\n",
    "    #add xticks and label\n",
    "    if i//5==1:\n",
    "        axes[i//5,i%5].set_xticks(np.arange(np.amin(relaventEnergyBands),\n",
    "                                            np.amax(relaventEnergyBands)+np.ptp(relaventEnergyBands)/noOfXticks,\n",
    "                                            np.ptp(relaventEnergyBands)/noOfXticks),\n",
    "                                  labels=np.round(np.arange(np.amin(relaventEnergyBands),\n",
    "                                                            np.amax(relaventEnergyBands)+np.ptp(relaventEnergyBands)/noOfXticks,\n",
    "                                                            np.ptp(relaventEnergyBands)/noOfXticks),\n",
    "                                                  0).astype('int'),\n",
    "                                  fontsize=baseFontSize*1.1)\n",
    "        axes[i//5,i%5].set_xlabel(\"KeV\",fontsize=baseFontSize*1.2)\n",
    "\n",
    "    #add yticks and label\n",
    "    if i%5==0:\n",
    "        axes[i//5,i%5].set_ylabel(\"log(Counts/min) ratioed\",fontsize=baseFontSize*1.2)\n",
    "\n",
    "    axes[i//5,i%5].set_yticks(np.arange(0,\n",
    "                                np.amax(learntElementalSpectra)+np.ptp(learntElementalSpectra)/noOfYticks,\n",
    "                                np.ptp(learntElementalSpectra)/noOfYticks),\n",
    "                      labels=np.round(np.arange(0,\n",
    "                                                np.amax(learntElementalSpectra)+np.ptp(learntElementalSpectra)/noOfYticks,\n",
    "                                                np.ptp(learntElementalSpectra)/noOfYticks),\n",
    "                                      2),\n",
    "                      fontsize=baseFontSize*1.1)\n",
    "#add a title\n",
    "figure.suptitle(f\"Element-wise Learnt spectra (Smoothened)\",\n",
    "                fontsize=baseFontSize*1.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6236b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
